{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d150b22",
   "metadata": {},
   "source": [
    "# Sparse Gaussian Process: an exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30eeb1",
   "metadata": {},
   "source": [
    "**Computational Statistics Project (2021-22). Jaime Enr√≠quez Ballesteros.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9456e",
   "metadata": {},
   "source": [
    "\"*Gaussian process (GP) models are flexible probabilistic nonparametric models for regression,\n",
    "classification and other tasks. Unfortunately they suffer from computational intractability for\n",
    "large data sets. Over the past decade there have been many different approximations\n",
    "developed to reduce this cost. Most of these can be termed global approximations, in that they\n",
    "try to summarize all the training data via a small set of support points. A different approach is\n",
    "that of local regression, where many local experts account for their own part of space. In this\n",
    "project we are interested to study the regimes in which these different approaches work well\n",
    "or fail, and then apply a new sparse GP approximation which is a combination of both the\n",
    "global and local approaches, and look extremely promising.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139a5b8",
   "metadata": {},
   "source": [
    "General imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb08a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPflow.gpflow as gpflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from GPflow.gpflow.config import default_float, default_jitter\n",
    "from GPflow.gpflow.utilities import print_summary, set_trainable, to_default_float\n",
    "from GPflow.gpflow.models import maximum_log_likelihood_objective, training_loss_closure\n",
    "from GPflow.gpflow.ci_utils import ci_niter\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6197052",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ea653",
   "metadata": {},
   "source": [
    "This project covers different techniques which might be used when wanting to use a Gaussian Process Regression while having a big dataset. As it will be shown, this is a big problem since as the dataset being used gets bigger, the problem becomes computationally intractable.\n",
    "\n",
    "The solution at hand is that of Sparse Gaussian Process Regression, which try to approximate the posterior distribution of the real Gaussian Process Regression model without loosing too much information about the data.Two main groups can be distinguished: local and global methods. Several models have been proposed which combine \"*the best of both worlds*\": a combination between both. All of these will be explained in the following sections.\n",
    "\n",
    "The project is focused in understanding GPR and SGPR, with the help of the articles by *Snelson and Ghahramani(2007)* and *Titsias (2009)* (found at the bottom of the document). The principal focus is made on the former. A reproducibility objective seems interesting with regard to this articles, as well as an expansion with an external database.\n",
    "\n",
    "The programming framework used to model Gaussian Processes is *GPFlow* (https://www.gpflow.org/): similar to GPyTorch, only implemented through TensorFlow.\n",
    "\n",
    "As a sidenote about the framework, we have to take into mind that not EVERY possibility is available among the huge world of Gaussian Processes. It is also a young framework, with little documentation.\n",
    "\n",
    "To compress these ideas in a single list of objectives, we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ac2137",
   "metadata": {},
   "source": [
    "**Objectives of the project:**\n",
    "1. Understand Sparse Gaussian Process Regression on big volume datasets.\n",
    "2. Understand how they are implemented in GPFlow library and the different models we can find.\n",
    "3. Try to reproduce experimentation on *Snelson & Ghahramani* (2007) and *Titsias* (2009) to understand how these models work.\n",
    "4. Use an external database(s) to confirm the characteristics of each method.\n",
    "5. Implement different schemes to place inducing points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c96fa",
   "metadata": {},
   "source": [
    "### General Functions implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b71360",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_univariate(model, color, ax, lims=[-1,11], show_xs=True, show_ind_locations=True):\n",
    "    x = model.data[0]\n",
    "    y = model.data[1]\n",
    "    xx = np.linspace(lims[0], lims[1], 100).reshape(-1, 1)\n",
    "    mu, var = model.predict_y(xx)\n",
    "    ax.plot(xx, mu, color, lw=2)\n",
    "    ax.fill_between(\n",
    "        xx[:, 0],\n",
    "        mu[:, 0] - 1.96 * np.sqrt(var[:, 0]),\n",
    "        mu[:, 0] + 1.96 * np.sqrt(var[:, 0]),\n",
    "        color=color,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    if show_xs:\n",
    "        ax.plot(x, y, \"k,\", mew=2)\n",
    "    if show_ind_locations:\n",
    "        try:\n",
    "            ax.plot(np.array(model.inducing_variable.variables[0]),\n",
    "                    np.repeat(min(mu[:, 0] - 1.96 * np.sqrt(var[:, 0])) - 1, N_ind),\n",
    "                    'rx')\n",
    "        except:\n",
    "            pass\n",
    "    # Plot title\n",
    "    ax.set_title(type(model).__name__)\n",
    "    ax.set_xlim(lims[0], lims[1])\n",
    "\n",
    "\n",
    "def create_models(data,\n",
    "                  inducing_variable,\n",
    "                  optimize=True,\n",
    "                  opt=gpflow.optimizers.Scipy(),  # BFGS\n",
    "                  niter=100,\n",
    "                  verbose=False\n",
    "                  ):\n",
    "\n",
    "    kernel = gpflow.kernels.SquaredExponential() # REVIEW: check other kernels\n",
    "    m1 = gpflow.models.GPR(data, kernel=kernel)\n",
    "\n",
    "    kernel = gpflow.kernels.SquaredExponential()\n",
    "    m2 = gpflow.models.SGPR(\n",
    "        data, kernel=kernel, inducing_variable=inducing_variable\n",
    "    )\n",
    "    set_trainable(m2.inducing_variable, False)\n",
    "\n",
    "    kernel = gpflow.kernels.SquaredExponential()\n",
    "    m3 = gpflow.models.GPRFITC(\n",
    "        data, kernel=kernel, inducing_variable=inducing_variable\n",
    "    )\n",
    "    set_trainable(m3.inducing_variable, False)\n",
    "\n",
    "    kernel = gpflow.kernels.Matern52()\n",
    "    m4 = gpflow.models.GPRPITC(\n",
    "        data, kernel=kernel, inducing_variable=inducing_variable\n",
    "    )\n",
    "    set_trainable(m4.inducing_variable, False)\n",
    "\n",
    "    kernel = gpflow.kernels.SquaredExponential()\n",
    "    m5 = gpflow.models.GPRPIC(\n",
    "        data, kernel=kernel, inducing_variable=inducing_variable\n",
    "    )\n",
    "    set_trainable(m5.inducing_variable, False)\n",
    "\n",
    "    models = [m1,m2,m3,m4,m5]\n",
    "\n",
    "    if verbose:\n",
    "        for model in models:\n",
    "            print_summary(model)\n",
    "\n",
    "    times = []\n",
    "    if optimize:\n",
    "        # Optimize models\n",
    "        for model in models:\n",
    "            print(\"Starting training of \", type(model).__name__)\n",
    "            loss_closure = training_loss_closure(model, data)\n",
    "            start = timer()\n",
    "            opt.minimize(\n",
    "                loss_closure,\n",
    "                variables=model.trainable_variables,\n",
    "                options=dict(maxiter=ci_niter(100))\n",
    "            )\n",
    "            end = timer()\n",
    "            print(\"Finished training of \", type(model).__name__)\n",
    "            times.append(end-start)\n",
    "\n",
    "\n",
    "    return models, times\n",
    "\n",
    "\n",
    "# m.predict_f -> returns the mean and variance of ùëì at the points Xnew\n",
    "# m.predict_f_samples -> returns samples of the latent function.\n",
    "# m.predict_y -> returns the mean and variance of a new data point (that is, it includes the noise variance).\n",
    "def mse_test(model, test_data):\n",
    "    test_set, test_labels = data\n",
    "    X_t = tf.convert_to_tensor(test_set, dtype=default_float())\n",
    "    mean, var = model.predict_f(X_t) \n",
    "    return mean_squared_error(mean, test_labels)\n",
    "\n",
    "\n",
    "# REVIEW: is this the correct formula?\n",
    "# m.predict_density -> returns the log density of the observations Ynew at Xnew.\n",
    "def nlpd_test(model, test_data):\n",
    "    N = len(test_data[0])\n",
    "    return 1/N*np.sum(model.predict_log_density(test_data))\n",
    "\n",
    "\n",
    "def error_vs_time(times, mses, nlpds,\n",
    "                  markers=['.', '.', 'o', '+'],\n",
    "                  colors=['black', 'red', 'blue', 'black'],\n",
    "                  mfcs=[False, False, True, False],\n",
    "                  model_names=['GPR', 'SGPR', 'FITC', 'PITC']):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "    # MSE vs time/s\n",
    "    for i in range(len(times)):\n",
    "        axs[0].scatter(times[i],\n",
    "                       mses[i],\n",
    "                       marker=markers[i],\n",
    "                       facecolors='none' if mfcs[i] else colors[i],\n",
    "                       edgecolors=colors[i],\n",
    "                       label=model_names[i])\n",
    "\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylabel(\"MSE\")\n",
    "    axs[0].set_xlabel(\"time/s\")\n",
    "    axs[0].set_title(\"MSE vs time\")\n",
    "\n",
    "    # NLPD vs time/s\n",
    "    for i in range(len(times)):\n",
    "        axs[1].scatter(times[i],\n",
    "                       nlpds[i],\n",
    "                       marker=markers[i],\n",
    "                       facecolors='none' if mfcs[i] else colors[i],\n",
    "                       edgecolors=colors[i],\n",
    "                       label=model_names[i])\n",
    "\n",
    "    axs[1].legend()\n",
    "    axs[1].set_ylabel(\"NLPD\")\n",
    "    axs[1].set_xlabel(\"time/s\")\n",
    "    axs[1].set_title(\"NLPD vs time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd4d868",
   "metadata": {},
   "source": [
    "## Introduction to Sparse Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476478a4",
   "metadata": {},
   "source": [
    "### Brief recap on Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c4ae5",
   "metadata": {},
   "source": [
    "$\\TODO$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0cdfa",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea37d60",
   "metadata": {},
   "source": [
    "$\\TODO$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fba895",
   "metadata": {},
   "source": [
    "### Why do we need Sparse GPR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbfb59",
   "metadata": {},
   "source": [
    "$\\TODO$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ce595",
   "metadata": {},
   "source": [
    "## Types of Sparse Gaussian Process Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72945188",
   "metadata": {},
   "source": [
    "From the multitude of papers and models one can find about Sparse GPR models, this project focuses on the proposals presented in *Snelson & Ghahramani (2007)*. Here, a distinction is made between:\n",
    "* **Global approaches**: where the idea behind is to use a set of support points which can summarize the whole dataset. A similar procedure to the original GPR is used using only the support points. This is the reason for the term *global*.\n",
    "\n",
    "* **Local approaches**: where data is grouped such that a GPR model is deployed for each of this blocks. The posterior distribution for a test point will correspond to the block to which this point belongs to. Such is, a *local* approach.\n",
    "\n",
    "In the paper mentioned above, a recap of two global methods are presented. These are the FIC (Fully Independent Conditional) and FITC (Fully Independent (Training) Conditional) approximations. Given a set of inducing points $\\overline{f}$, the GP prior is expressed as:\n",
    "\n",
    "$$\n",
    "p(f,f_t) = \\int d\\overline{f} \\text{  } p(f,f_T | \\overline{f})  \\text{  } p(\\overline{f})\n",
    "$$\n",
    "\n",
    "From here, it is assumed that $f$ and $f_T$ are independent given $\\overline{f}$:\n",
    "\n",
    "$$\n",
    "p(f,f_T) \\approx q(f,f_T) = \\int d\\overline{f} \\text{  } q(f_T | \\overline{f}) \\text{  } q(f | \\overline{f})  \\text{  } p(\\overline{f})\n",
    "$$\n",
    "\n",
    "The FITC and FIC differ on the different following assumptions made on the training and test conditionals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9f226",
   "metadata": {},
   "source": [
    "### FIC\n",
    "For the first of the Fully Independent global approximations, the assumption is made that both the training and test conditionals are fully independent s.t. $q(f|\\overline{f})=\\prod\\limits_n{p(f_n|\\overline{f})}$ and $q(f_T|\\overline{f})=\\prod\\limits_t{p(f_t|\\overline{f})}$. \n",
    "\n",
    "As a result the prior and predictive distributions found are the ones found in the first row of Table 1.\n",
    "\n",
    "### FITC\n",
    "This second global approximation only considers the *training* data points to be independent. We find the same conditional for training points as before: $q(f|\\overline{f})=\\prod\\limits_n{p(f_n|\\overline{f})}$; but, now, the test conditional: $q(f_T|\\overline{f})=p(f_T|\\overline{f})$.\n",
    "\n",
    "This will transform into a different covariance prior in the testing points slots, as shown in Table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad6c37",
   "metadata": {},
   "source": [
    "![Table 1](img/Table_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c55ef",
   "metadata": {},
   "source": [
    "### Local approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f8a9bb",
   "metadata": {},
   "source": [
    "$\\TODO$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae3e4e",
   "metadata": {},
   "source": [
    "### Global + Local approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44a118",
   "metadata": {},
   "source": [
    "Both global and local approaches present very good qualities. They also show disadvantages which are, somehow, complementary of each other. This is the main motivation to search for a method which combines both approaches.\n",
    "\n",
    "As a first proposal, in *Snelson & Ghahramani (2007)* we find the PITC (Partially Independent Training Conditional).\n",
    "\n",
    "### PITC\n",
    "\n",
    "First proposed in *Qui√±onera and Rasmussen (2005)*, the PITC method follows the steps of FITC. As a first step, the training points are divided into blocks $  X_{B_s}, f_{B_s} $ being $s \\in S$ each of the blocks.\n",
    "\n",
    "PITC assumes *partial independence* for the training points. Independence is only considered for points which do not belong to the same block. Following the same notation as FI(T)C, the training and test conditionals are: $q(f|\\overline{f})=\\prod\\limits_s^S{p(f_{B_s}|\\overline{f})}$ and $q(f_T|\\overline{f})=p(f_T|\\overline{f})$.\n",
    "\n",
    "These conditionals probabilities make us understand that the covariance prior will have to be composed by a *block-diagonal* taken of the original training points. The blocks of the block-diagonal matrix being covariance sub-matrices of the original covariance matrix (minus the new sparse diagonal $Q_N$). The corresponding formulas for the predictive distribution (on test point $f_*$) are shown in Table 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af6edd",
   "metadata": {},
   "source": [
    "### PIC\n",
    "\n",
    "A new approach is developed in *Snelson & Ghahramani (2007)* following the steps of PITC. The motivation for it is that PITC is still to similar to FITC (as we can see by comparing the predictive distributions). One can not expect too much difference between the two. Another important issue is that the blocking scheme in the training points marginalizes the testing points into a block of their own. Meaning that the test points are related to the training points only through the inducing points. An issue we wanted to avoid when combining global and local methods.\n",
    "\n",
    "The new approach, named PIC, treats training and testing points and considers them jointly for the blocking scheme. This results in a joint conditional: $q(f, f_T|\\overline{f})=p(f_{B_s}, f_T|\\overline{f}) \\prod\\limits_s^{S-1}{p(f_{B_s}|\\overline{f})}$.\n",
    "\n",
    "The corresponding predictive distribution is shown in Table 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb38ba",
   "metadata": {},
   "source": [
    "![](img/Table_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61263587",
   "metadata": {},
   "source": [
    "Of course, two questions come to mind about the methods shown above.\n",
    "* **Where to put / How many inducing points** to obtain the best model possible?\n",
    "* **How big / How many blocks** should be appointed?\n",
    "\n",
    "Usually, the location of inducing points is decided randomly. Although an interesting proposal is choosing the corresopnding location of inducing points based on the clusters formed in the local-part of the method.\n",
    "\n",
    "Regarding how many inducing points to appoint, the only requirement is that this number should be much smaller than the original number of data points ($M<<N$).\n",
    "\n",
    "The size of the blocks is usually the same for every block. So the size and number of blocks usually have a direct relation. According to *Qui√±onera and Rasmussen (2005)*, \"*A reasonable choice, recommended by Tresp (1999) may be to choose k = n/m blocks.*\" i.e. the number of total points divided by the number of inducing points.\n",
    "\n",
    "It is important to mention that when the number of inducing points or blocks in the model is increased greatly, it will tend to a GP Regresion model (since $Q=K$). \n",
    "On the other spectrum, when block sizes tend to one we obtain the FIC model. Furthermore, when the inducing points tend to zero, we are left with the original local GP predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf14afe",
   "metadata": {},
   "source": [
    "$\\TODO$\n",
    "* Table of complexities\n",
    "* Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204c2ad",
   "metadata": {},
   "source": [
    "## Implementation of Local, PITC and PIC methods in GPFlow context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e235372e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7bc22ee",
   "metadata": {},
   "source": [
    "## Simple Guassian Regression Example (d=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c966c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "np.random.seed(42)\n",
    "N = 1000\n",
    "N_t = 100\n",
    "N_ind = 10\n",
    "X = np.random.rand(N, 1) * 10\n",
    "Y = np.sin(X) + 0.9 * np.cos(X * 1.6) + np.random.randn(*X.shape) * 0.4\n",
    "Xtest = np.random.rand(N_t, 1) * 10\n",
    "# _ = plt.plot(X, Y, \"kx\", mew=2)\n",
    "# plt.show()\n",
    "\n",
    "data = (\n",
    "    tf.convert_to_tensor(X, dtype=default_float()),\n",
    "    tf.convert_to_tensor(Y, dtype=default_float()),\n",
    ")\n",
    "X_ind = X[0:N-1:int(N/N_ind)]\n",
    "inducing_variable = tf.convert_to_tensor(X_ind, dtype=default_float())\n",
    "\n",
    "# Create models\n",
    "models, _ = create_models(data, inducing_variable)\n",
    "\n",
    "# GPR, SGPR, FITC, PITC, PIC = models  # REVIEW: find out what is so slow in PITC (see testing area)\n",
    "GPR,SGPR,FITC,PITC = models\n",
    "\n",
    "# Plot results\n",
    "f, ax = plt.subplots(3, 2, figsize=(12, 9), sharex=False, sharey=False)\n",
    "plot_univariate(GPR, \"C0\", ax[0, 0], show_xs=True)\n",
    "plot_univariate(SGPR, \"C1\", ax[0, 1], show_xs=True)\n",
    "plot_univariate(FITC, \"C2\", ax[1, 0], show_xs=True)\n",
    "plot_univariate(PITC, \"C3\", ax[1, 1], show_xs=True, lims=[-1,6])\n",
    "# plot_univariate(PIC, \"C4\", ax[2, 1], show_xs=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18c3be",
   "metadata": {},
   "source": [
    "## Comparison between Guassian Process Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97560816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "213293fd",
   "metadata": {},
   "source": [
    "## Reproduction of previous experiments found in *Snelson & Ghahramani* (2007) and *Titsias* (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08db82",
   "metadata": {},
   "source": [
    "### Kin40k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ab910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57a677ae",
   "metadata": {},
   "source": [
    "### Abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a97962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5f95b4",
   "metadata": {},
   "source": [
    "### SARCOS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97990b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0311d83d",
   "metadata": {},
   "source": [
    "## External dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8c510",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bdbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ebddfc8",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568bc75",
   "metadata": {},
   "source": [
    "1. Snelson and Ghahramani. (2007). Local and global sparse Gaussian process approximations. Artificial Intelligence and Statistics 11 (AISTATS).http://proceedings.mlr.press/v2/snelson07a/snelson07a.pdf\n",
    "2. Titsias. (2009). Variational Learning of Inducing Variables in Sparse Gaussian Processes. Journal of Machine Learning Research - Proceedings Track. 5. 567-574. http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf\n",
    "3. Qui√±onera and Rasmussen (2005). A Unifying View of Sparse Approximate Gaussian Process Regression.https://www.jmlr.org/papers/v6/quinonero-candela05a.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
